\section{Literature Review}
% =========================================================
% NEW SECTION: DATA DISTRIBUTION SHIFTS
% =========================================================

\section{Data Distribution Shifts}
\label{sec:data_drift}

In the context of integrating historical or frame data with current survey samples, the assumption that the training distribution (history) matches the target distribution (current period) is rarely satisfied. While \textit{data distribution shift} is often used as a blanket term, it comprises three distinct subtypes: \textit{Covariate Shift}, \textit{Label Shift}, and \textit{Concept Drift} \parencite{moreno_torres_2012}. To formally define these shifts, let $\mathcal{X}$ denote the input space and $\mathcal{Y}$ the output space. We view the data generation process as drawing samples from a joint distribution $P(\mathbf{x}, y)$. 

Standard machine learning models estimate the conditional probability $P(y \mid \mathbf{x})$. The joint distribution can be decomposed in two ways, corresponding to different causal perspectives:
\begin{equation}
\label{eq:joint_decomp}
P(\mathbf{x}, y) \;=\; 
\underbrace{P(y \mid \mathbf{x})}_{\text{Mechanism}} \underbrace{P(\mathbf{x})}_{\text{Covariates}} 
\;=\; 
\underbrace{P(\mathbf{x} \mid y)}_{\text{Manifestation}} \underbrace{P(y)}_{\text{Prior}}.
\end{equation}

\noindent We categorize shifts based on which components of these decompositions change between the source (historical, $P_S$) and target (current, $P_T$) distributions.

\paragraph{Covariate Shift.}
Covariate shift occurs when the distribution of inputs changes, but the relationship between inputs and outputs remains fixed \parencite{shimodaira_2000_jspi}. Mathematically:
\begin{equation}
P_S(\mathbf{x}) \neq P_T(\mathbf{x}) \quad \text{but} \quad P_S(y \mid \mathbf{x}) = P_T(y \mid \mathbf{x}).
\end{equation}
This corresponds to a shift in the second term of the first decomposition in \eqref{eq:joint_decomp}. In survey sampling, this is analogous to a change in the auxiliary population structure (e.g., a demographic shift in $\mathbf{z}$) while the economic mechanism $y \mid \mathbf{x}$ remains stable.

\paragraph{Label Shift.}
Label shift, also referred to as \textit{prior probability shift}, occurs when the marginal distribution of the labels changes, but the class-conditional density remains invariant \parencite{lipton_2018_labelshift}.
\begin{equation}
P_S(y) \neq P_T(y) \quad \text{but} \quad P_S(\mathbf{x} \mid y) = P_T(\mathbf{x} \mid y).
\end{equation}
This aligns with the second decomposition in \eqref{eq:joint_decomp} and is common in "anti-causal" problems where the outcome causes the features (e.g., a disease $y$ causing symptoms $\mathbf{x}$). If an epidemic increases the prevalence $P(y)$, the observed feature distribution $P(\mathbf{x})$ shifts solely due to the change in $P(y)$.

\paragraph{Concept Drift.}
Concept drift (or \textit{posterior shift}) implies a fundamental change in the data-generating mechanism itself \parencite{gama_2014_surveydrift}:
\begin{equation}
P_S(y \mid \mathbf{x}) \neq P_T(y \mid \mathbf{x}) \quad \text{but} \quad P_S(\mathbf{x}) = P_T(\mathbf{x}).
\end{equation}
Here, the conditional distribution of the output changes for the same input. For example, the relationship between housing features $\mathbf{x}$ and price $y$ may fundamentally change after a macroeconomic shock (e.g., COVID-19), such that a model trained on $P_S$ yields biased predictions under $P_T$ even if the housing stock $\mathbf{x}$ remains constant.

\subsection{Detection of Distribution Shifts}
Detecting shift requires comparing the source and target distributions using statistical hypothesis testing. While simple summary statistics (e.g., drift in mean or variance) provide heuristics, rigorous detection utilizes two-sample tests.

\paragraph{Univariate Testing.}
For one-dimensional data (e.g., scalar predictions or labels), the \textbf{Kolmogorov-Smirnov (KS) Test} is a standard non-parametric method \parencite{massey_1951_kstest}. It quantifies the distance between the empirical cumulative distribution functions (ECDF) of the source and target data. The null hypothesis $H_0: P_S = P_T$ is rejected if the KS statistic exceeds a critical threshold derived from the reference distribution.

\paragraph{Multivariate Testing.}
For high-dimensional vectors $\mathbf{x}$, the KS test is inapplicable. Instead, we employ kernel-based measures such as the \textbf{Maximum Mean Discrepancy (MMD)} \parencite{gretton_2012_jmlr}. MMD maps distributions to a Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$ via a feature map $\phi(\cdot)$. The squared MMD distance is defined as:
\begin{equation}
\text{MMD}^2(P_S, P_T) \;=\; \left\| \mathbb{E}_{\mathbf{x} \sim P_S}[\phi(\mathbf{x})] - \mathbb{E}_{\mathbf{x} \sim P_T}[\phi(\mathbf{x})] \right\|_{\mathcal{H}}^2.
\end{equation}
An alternative approach is \textbf{Least Squares Density Difference (LSDD)} \parencite{sugiyama_2013_density}, which directly estimates the density difference function without estimating the densities themselves. To improve test power, dimensionality reduction is often recommended prior to applying these two-sample tests.

\subsection{Addressing Distribution Shifts}
Strategies to mitigate distribution shift generally fall into three categories:

\paragraph{1. Importance Weighting (for Covariate Shift).}
If the shift is strictly covariate ($P_S(y|\mathbf{x}) = P_T(y|\mathbf{x})$), one can correct the bias by re-weighting the training samples. The optimal weights correspond to the density ratio \parencite{shimodaira_2000_jspi}:
\begin{equation}
w(\mathbf{x}) \;=\; \frac{P_T(\mathbf{x})}{P_S(\mathbf{x})}.
\end{equation}
The model is then trained by minimizing the weighted loss $\sum_i w(\mathbf{x}_i) \ell(f(\mathbf{x}_i), y_i)$, effectively forcing the training distribution to mimic the target distribution.

\paragraph{2. Invariant Representation Learning.}
Rather than re-weighting, this class of methods seeks a feature transformation $\Phi(\mathbf{x})$ such that the distribution of mapped features is invariant across domains. \textcite{zhang_2013_icml} proposed using causal interpretations with kernel embeddings to correct for both target (label) and conditional shifts without requiring target labels. Similarly, \textcite{zhou_2021_domaininv} proposed \textit{domain-invariant representation learning}, an unsupervised adaptation technique that learns representations invariant to density transformations between domains. While theoretically powerful, these methods assume that a shared invariant structure exists and often require complex adversarial training procedures.

\paragraph{3. Transfer Learning and Fine-tuning.}
The standard industrial approach treats the shift as a Transfer Learning problem \parencite{pan_yang_2010_transfer}. Here, the parameters $\theta_S$ learned on the source distribution $P_S$ serve as the initialization for a model on $P_T$. The model is then \textit{fine-tuned} (retrained) using a small labeled sample from the target distribution. This effectively combines the general features learned from the massive source dataset with the specific mechanism of the target domain.

\medskip
\noindent In the context of our proposed \textbf{Hist-PPD-DR estimator} (Scenario 5), we effectively adopt a hybrid of these approaches: the historical model $f_{\text{hist}}$ acts as a transferred base learner (Approach 3), while the mass-imputation term $\sum (\tilde{y}_i - f_{\text{hist}})$ explicitly corrects for Concept Drift, and the propensity weights $1/\hat{p}_i$ perform a finite-population analogue of Importance Weighting (Approach 1) to handle the item nonresponse bias.


\section{Prediction-Powered Inference (PPI): Notes}

\begin{algorithm}[H]
\caption{Prediction-powered mean estimation (i.i.d. version)}
\label{alg:mean_estimation}
\footnotesize
\begin{algorithmic}[1]
\State \textbf{Input:} labeled data $(X,Y)$, unlabeled features $\tilde{X}$, predictor $f$, error level $\alpha \in (0,1)$
\State $\hat{\theta}^{\text{PP}} \leftarrow \tilde{\theta}^f - \hat{\Delta} := \frac{1}{N}\sum_{i=1}^{N} f(\tilde{X}_i) - \frac{1}{n}\sum_{i=1}^{n} (f(X_i) - Y_i)$
\State $\hat{\sigma}_{\tilde{f}}^2 \leftarrow \frac{1}{N}\sum_{i=1}^{N}(f(\tilde{X}_i) - \tilde{\theta}^f)^2$
\State $\hat{\sigma}_{f-Y}^2 \leftarrow \frac{1}{n}\sum_{i=1}^{n}(f(X_i) - Y_i - \hat{\Delta})^2$
\State $w_{\alpha} \leftarrow z_{1-\alpha/2} \sqrt{\frac{\hat{\sigma}_{f-Y}^2}{n} + \frac{\hat{\sigma}_{\tilde{f}}^2}{N}}$
\State \textbf{Output:} $C_{\alpha}^{\text{PP}} = (\hat{\theta}^{\text{PP}} \pm w_{\alpha})$
\end{algorithmic}
\end{algorithm}

\begin{itemize}
  \item \textbf{Expensive (gold) measurements:} a labeled set $(X,Y)=\{(X_i,Y_i)\}_{i=1}^n$ of size $n$ where $Y$ is observed.
  \item \textbf{Cheap measurements (on the same $X$):} model predictions $f(X_i)$ for $i\in[n]$ computed by a \emph{fixed} rule $f:\mathcal X\!\to\!\mathbb R$. These are \emph{coupled} with the gold labels because they are predictions for the very same inputs $\{X_i\}_{i=1}^n$; we use them only to quantify and correct prediction error.
  \item \textbf{Large pool of cheap measurements:} predictions on a large unlabeled feature set $\tilde X=\{\tilde X_i\}_{i=1}^N$ with $N\gg n$, i.e., $f(\tilde X)=\{f(\tilde X_i)\}_{i=1}^N$.
\end{itemize}

We treat $f$ as exogenous for inference (e.g., trained on independent data or obtained by sample-splitting). The estimand for this warm-up is the population mean outcome $\theta^*=\mathbb E[Y]$.
The target is a property of the population distribution $P$, e.g., the mean outcome $\estimand{\theta^*=\mathbb E[Y]}$, and the inferential goal is a \emph{valid $(1-\alpha)$ confidence interval} for $\theta^*$; the point estimate is a means to that end.


\paragraph{Classical vs.\ PPI mean estimators.}
The classical estimator uses only gold labels:
\begin{equation}
\hat\theta^{\text{class}}=\frac{1}{n}\sum_{i=1}^n Y_i.
\label{eq:classical-mean}
\end{equation}
PPI splits the job into (i) an \emph{imputation on cheap data} and (ii) a \emph{rectifier} learned on gold data:
\begin{align}
\predonly{\tilde\theta^{\,f}} &:= \frac{1}{N}\sum_{i=1}^{N} f(\tilde X_i), \tag{1.1}\\
\rectifier{\hat\Delta} &:= \frac{1}{n}\sum_{i=1}^{n}\bigl(f(X_i)-Y_i\bigr), \tag{1.2}\\
\ppi{\hat\theta^{\mathrm{PP}}} &:= \tilde\theta^{\,f}-\hat\Delta \\
&= \frac{1}{N} \sum_{i=1}^{N} f(\tilde{X}_i) - \frac{1}{n} \sum_{i=1}^{n} \left( f(X_i) - Y_i \right). \tag{1.3}
\end{align}
Intuition: if $f$ is accurate on the gold set, then $\hat\Delta\approx 0$ and $\hat\theta^{\mathrm{PP}}\approx \frac{1}{N}\sum_{i=1}^N \tilde Y_i$, which enjoys much lower variance when $N\!\gg\! n$.

With
\[
\predonly{\hat\sigma_f^2}:=\frac{1}{N}\sum_{i=1}^N\bigl(f(\tilde X_i)-\tilde\theta^{\,f}\bigr)^2,\qquad
\rectifier{\hat\sigma^2_{f-Y}}:=\frac{1}{n}\sum_{i=1}^n\bigl(f(X_i)-Y_i-\hat\Delta\bigr)^2,
\]
a normal approximation yields the $(1-\alpha)$ PPI interval
\begin{equation}
\label{eq:iid-ci}
\ppi{C_\alpha^{\mathrm{PP}}=\Bigl[\hat\theta^{\mathrm{PP}}\pm z_{1-\alpha/2}\sqrt{\frac{\rectifier{\hat\sigma^2_{f-Y}}}{n}+\frac{\predonly{\hat\sigma_f^2}}{N}}\Bigr]}. \tag{1.4}
\end{equation}
For comparison, the classical interval is
\begin{equation}
C_\alpha^{\text{class}}=
\Bigl[\hat\theta^{\text{class}}\pm z_{1-\alpha/2}\sqrt{\frac{\widehat{\mathrm{Var}}(Y)}{n}}\Bigr].
\label{eq:classical-ci}
\end{equation}
When $f$ predicts well, $\hat\Delta\approx 0$, the width of~\eqref{eq:iid-ci} is dominated by $\rectifier{\hat\sigma^2_{f-Y}}/n$ and is typically much smaller than~\eqref{eq:classical-ci}.


% =========================================================
\paragraph{General Convex Estimand Framework}
Let $\ell_\theta$ be a convex loss and define 
\begin{equation}
    \estimand{\theta^* = \arg\min_{\theta \in \mathbb{R}^p} \mathbb{E}[\ell_{\theta}(X, Y)]}. \tag{2.1}
\end{equation}
With (sub)gradient $g_\theta=\partial_\theta\ell_\theta$, define the rectifier
\begin{equation}
\rectifier{\Delta_\theta} \;=\; \mathbb E\big[ g_\theta(X,Y) - g_\theta(X,f(X)) \big].
\end{equation}
For each $\theta$, form two confidence sets using off-the-shelf mean CI’s: (i) $R_\delta(\theta)$ for $\Delta_\theta$ from labeled data; (ii) $T_{\alpha-\delta}(\theta)$ for $g^f_\theta:=\mathbb E[g_\theta(X,f(X))]$ from unlabeled predictions.
\[
\mathbb{E}[g_{\theta}(X,Y)] = \predonly{\mathbb{E}[g_{\theta}(X, f(X))]} + \rectifier{\mathbb{E}[g_{\theta}(X,Y) - g_{\theta}(X,f(X))]} = 0.
\]
Then define the \emph{prediction-powered confidence set}
\begin{equation}
\ppi{C_\alpha^{\mathrm{PP}}} \;=\; \Big\{\theta:\; 0\in R_\delta(\theta) + T_{\alpha-\delta}(\theta)\Big\},
\end{equation}
where $+$ denotes the Minkowski sum. Under a mild nondegeneracy condition, $\mathbb P(\theta^*\in C_\alpha^{\mathrm{PP}})\ge 1-\alpha$.




\section{Overview of the Prediction-Powered Bootstrap (PPBoot)}

\paragraph{Setting.}
We observe a small labeled sample \((X,Y)=\{(X_i,Y_i)\}_{i=1}^n\) and a large unlabeled sample \(\tilde X=\{\tilde X_j\}_{j=1}^N\) with \(N \gg n\). A fixed, data-independent predictor \(f:\mathcal X\to\mathcal Y\) provides \(f(X)\) and \(f(\tilde X)\).
Our inferential target is a generic estimand \(\theta^\star\), and \(\hat\theta(\cdot)\) denotes any standard plug-in estimator for \(\theta^\star\) when outcomes are observed (e.g., means, quantiles, regression coefficients). 

\paragraph{Core idea (functional form).}
PPBoot builds a bootstrap replicate that \emph{adds} an imputed estimate from the large unlabeled set and \emph{subtracts} the model's contribution on the small labeled set so that prediction error cancels to first order:
\[
\theta_b^{\ast}
\;=\;
\hat\theta\!\big(\tilde X^{\ast},\, f(\tilde X^{\ast})\big)
\;+\;
\hat\theta\!\big(X^{\ast}, Y^{\ast}\big)
\;-\;
\hat\theta\!\big(X^{\ast}, f(X^{\ast})\big),
\qquad b=1,\dots,B,
\]
where, at each bootstrap iteration \(b\), we resample with replacement
\((X^{\ast},Y^{\ast})\) from \((X,Y)\) and \(\tilde X^{\ast}\) from \(\tilde X\).
Applying the percentile rule to \(\{\theta_b^{\ast}\}_{b=1}^B\) yields the PPBoot confidence interval
\[
\CPPBoot
=
\Big[
\quantile\!\big(\{\theta_b^\ast\}_{b=1}^B;\alpha/2\big),\;
\quantile\!\big(\{\theta_b^\ast\}_{b=1}^B;1-\alpha/2\big)
\Big].
\]

\paragraph{How it works (mechanics).}
\begin{itemize}
  \item \textbf{Resample two sources:} draw \( (X^{\ast},Y^{\ast}) \) from the labeled pairs and \( \tilde X^{\ast} \) from the unlabeled features, independently, at each bootstrap iteration.
  \item \textbf{Compute a bias-corrected replicate:}
  \(\hat\theta(\tilde X^{\ast}, f(\tilde X^{\ast}))\) is a high-throughput, imputed estimate using all (pseudo-)unlabeled examples; 
  \(\hat\theta(X^{\ast}, Y^{\ast})-\hat\theta(X^{\ast}, f(X^{\ast}))\) is a \emph{rectifier} measured on the labeled data that removes bias introduced by replacing \(Y\) with \(f(X)\).
  \item \textbf{Percentile interval:} summarize the empirical distribution of \(\{\theta_b^\ast\}\) with the \(\alpha/2\) and \(1-\alpha/2\) quantiles.
\end{itemize}

\paragraph{Why it works (intuition).}
The replicate is a bootstrap analogue of a \emph{model-assisted} correction:
if \(f\) is reasonably accurate, then on the labeled resample
\(\hat\theta(X^{\ast}, f(X^{\ast})) \approx \hat\theta(X^{\ast}, Y^{\ast})\), so the correction term is near zero and
\[
\theta_b^{\ast}\;\approx\;\hat\theta\!\big(\tilde X^{\ast}, f(\tilde X^{\ast})\big).
\]
That imputed piece leverages \(N \gg n\) examples, giving low variance.
When \(f\) is imperfect, the labeled-data difference
\(\hat\theta(X^{\ast}, Y^{\ast})-\hat\theta(X^{\ast}, f(X^{\ast}))\) \emph{de-biases} the unlabeled imputation at each bootstrap draw.
This yields a bootstrap distribution concentrated like the large-\(N\) imputation but centered correctly by the small labeled set—hence intervals that are (i) valid and (ii) typically much shorter than classical, labeled-only bootstrap intervals.

\paragraph{What you need to run it.}
\begin{itemize}
  \item A black-box prediction rule \(f\) independent of \((X,Y),\tilde X\).
  \item Any estimator \(\hat\theta\) you would normally use if outcomes were observed (means, quantiles, GLM coefficients, M-estimators, etc.).
  \item A bootstrap size \(B\) (hundreds to a few thousands are typical); resample labeled and unlabeled pools \emph{separately with replacement}.
\end{itemize}

\paragraph{Algorithm (\PPBoot).}
\begin{enumerate}
  \item For \(b=1,\dots,B\):
    \begin{enumerate}
      \item Resample \((X^{\ast},Y^{\ast})\) from \((X,Y)\) and \(\tilde X^{\ast}\) from \(\tilde X\) (with replacement).
      \item Compute \(\displaystyle \theta_b^{\ast}=\hat\theta(\tilde X^{\ast}, f(\tilde X^{\ast}))+\hat\theta(X^{\ast},Y^{\ast})-\hat\theta(X^{\ast}, f(X^{\ast}))\).
    \end{enumerate}
  \item Return \(\displaystyle \CPPBoot=\big[\quantile(\{\theta_b^{\ast}\};\alpha/2),\;\quantile(\{\theta_b^{\ast}\};1-\alpha/2)\big]\).
\end{enumerate}

\paragraph{When to expect gains.}
Whenever \(f\) reduces outcome noise (so that \(f(X)\) is close to \(Y\)) and the unlabeled pool is large, the PPBoot replicates behave roughly like large-\(N\) estimators while the rectifier preserves validity. In short: \emph{better predictions \(\Rightarrow\) narrower intervals}, with no modeling assumptions required for \(f\) or \(\hat\theta\).










\subsection{Major estimators for item nonresponse (population total)}

This section reviews widely used estimators for the finite–population total in the presence of item nonresponse, with a focus on designs common in official and social surveys. Throughout, let $U$ denote a finite population of size $N$, and let $S\subseteq U$ be a probability sample of size $n$ with design inclusion (strictly positive)probabilities $\pi_i$ and corresponding design weights $d_i=1/\pi_i$. For unit $i\in U$, let $y_i$ be the study variable of interest (population total $T_y=\sum_{i\in U} y_i$). Let $\mathbf v_i$ denote auxiliary variables observed for all sampled units (available on $s$), and let $\mathbf z_i$ denote auxiliary variables observed for all population units (available on $U$; e.g., frame or administrative variables). The response indicator for $y_i$ is $r_i\in\{0,1\}$, with $r_i=1$ if $y_i$ is observed and $r_i=0$ otherwise; respondents and nonrespondents are $S_r=\{i\in s:r_i=1\}$ and $S_m=\{i\in s:r_i=0\}$. We adopt the standard \emph{missing at random given $\mathbf v_i$ (MAR)} assumption, $\Pr(r_i=1\mid \mathbf v_i,y_i)=\Pr(r_i=1\mid \mathbf v_i)\equiv \rho(\mathbf v_i)$, and write $\hat\rho_i=\widehat{\Pr}(r_i=1\mid \mathbf v_i)$ for a fitted response–propensity model (typically a design-weighted logistic/probit; cf.\ \parencite{little_1986_isr,kott_1994_jasa,kott_2006_surveymethodology}). When an outcome model $m(\mathbf v;\beta)$ for $E(y\mid \mathbf v)$ is fit on respondents (with design weights), we denote the prediction by $\tilde y_i=\widehat m_s(\mathbf v_i)$. When an externally trained or frame-based assisting model is available, we write $f(\mathbf z_i)$ and treat $\sum_{i\in U} f(\mathbf z_i)$ as fixed with respect to the current sample draw.

\paragraph{Horvitz--Thompson (HT). \parencite{horvitz_thompson_1952_jasa}}
With full response, the design–unbiased estimator of $T_y$ is 
$$\widehat T_{HT}=\sum_{i\in s} d_i\,y_i.$$
In the presence of item nonresponse, $\widehat T_{HT}$ is not directly computable because some $y_i$ are missing, but it serves as the benchmark to which adjusted estimators are compared. In the case of non-respondents, a naive and biased approach (not used in practice) would use only respondents with their original design weights,
\begin{equation}
  \widehat T_{\text{naive}}
  \;=\;
  \sum_{i\in S_r} d_i\,y_i.
\end{equation}
Because it ignores systematic differences between respondents and nonrespondents, $\widehat T_{\text{naive}}$ is unbiased and underestimates $T_y$ when response depends on $\mathbf v_i$ or $y_i$.

\paragraph{Propensity-weight (inverse-probability) adjustment.}
Weight-adjustment methods inflate respondent weights by the inverse of the estimated response probability $\hat\rho_i=\hat\rho(\mathbf v_i;\hat\alpha)$ fit on $(r_i,\mathbf v_i)$ for all sampled units (with design weights). The adjusted total is
\begin{equation}
  \widehat T_{PW}
  \;=\;
  \sum_{i\in S_r} \frac{d_i}{\hat\rho_i}\,y_i.
  \label{eq:PW}
\end{equation}
Intuitively, the weight of nonrespondents is redistributed to similar respondents defined by $\mathbf v_i$. Under MAR and correct specification of the response model (plus positivity, $\inf_i \rho(\mathbf v_i)>0$) and with auxiliary $\mathbf v_i$ observed for all sampled units, $\widehat T_{PW}$ is consistent for $T_y$ \parencite{little_1986_isr,kott_1994_jasa,kott_2006_surveymethodology}.

\paragraph{Calibration and generalized regression (difference/PPD) estimators.}
Calibration adjusts respondent weights so that weighted auxiliary totals match known benchmarks (either population totals of $\mathbf z$ or full-sample HT estimates). Let $d_i^\ast$ satisfy
\begin{equation}
  \sum_{i\in S_r} d_i^\ast\,\mathbf z_i
  \;=\;
  \sum_{i\in s} d_i\,\mathbf z_i
  \quad\text{(or }=\sum_{i\in U}\mathbf z_i\text{ when frame totals are known)}.
  \label{eq:calib-constraint}
\end{equation}
The calibrated total is
\begin{equation}
  \widehat T_{\mathrm{calib}}
  \;=\;
  \sum_{i\in S_r} d_i^\ast\,y_i.
  \label{eq:calib}
\end{equation}
Under a working linear model $E(y_i\mid\mathbf z_i)=\mathbf z_i^\top\beta$, \eqref{eq:calib} is numerically equivalent to the generalized regression (GREG) or \emph{difference} estimator (a.k.a.\ prediction–plus–difference, PPD)
\begin{equation}
  \widehat T_{reg}
  \;=\;
  \underbrace{\sum_{i\in U} f(\mathbf z_i)}_{\text{assisting prediction total}}
  \;+\;
  \underbrace{\sum_{i\in s} d_i\{y_i - f(\mathbf z_i)\}}_{\text{design-weighted difference}},
  \label{eq:PPD}
\end{equation}
where, in the linear-assisting case, $f(\mathbf z_i)=\mathbf z_i^\top\hat\beta$ with $\hat\beta$ estimated by (design-weighted) regression on respondents, and $\sum_{i\in U} f(\mathbf z_i)$ is treated as fixed when $f$ is trained externally on frame or historical data. If the working regression is correct, \(\widehat T_{reg}\) is model-unbiased; under complex designs, calibration also yields small design bias when $\mathbf z_i$ is informative for $y_i$ or response \parencite{deville_sarndal_1992_jasa,sarndal_lundstrom_2005_nonresponse,cochran_1977_sampling,isaki_fuller_1982_jasa}.

\paragraph{Model-based imputation (outcome modeling).}
Imputation replaces missing outcomes by predictions from an outcome model fit on respondents. Let $m(\mathbf v;\beta)$ be a working model for $E(y\mid\mathbf v)$ estimated on $\{(y_i,\mathbf v_i):i\in S_r\}$ (with design weights), and define $\tilde y_i=\widehat m_s(\mathbf v_i)$. The total estimator is
\begin{equation}
  \widehat T_{IMP}
  \;=\;
  \sum_{i\in S_r} d_i\,y_i
  \;+\;
  \sum_{i\in S_m} d_i\,\tilde y_i.
  \label{eq:IMP}
\end{equation}
Consistency requires correct specification of $m(\mathbf v;\beta)$ and availability of $\mathbf v_i$ for all sampled units; random or fractional hot-deck variants primarily affect variance rather than the point estimator in \eqref{eq:IMP} \parencite{little_rubin_2002_samd,sarndal_lundstrom_2005_nonresponse,haziza_2009_review,kim_fuller_2004_biometrika}.

\paragraph{Doubly robust (augmented IPW) estimators.}
Doubly robust (DR) estimators combine an outcome model and a response-propensity model so that consistency holds if \emph{either} component is correctly specified. A canonical design-based AIPW total is
\begin{equation}
  \widehat T_{DR}
  \;=\;
  \sum_{i\in s} d_i\,\tilde y_i
  \;+\;
  \sum_{i\in S_r}\frac{d_i}{\hat\rho_i}\,\bigl(y_i-\tilde y_i\bigr),
  \label{eq:DR}
\end{equation}
with $\tilde y_i=\widehat m_s(\mathbf v_i)$ fit on respondents and $\hat\rho_i=\widehat{\Pr}(r_i=1\mid\mathbf v_i)$ fit on all sampled units. The first term forms a design-weighted, model-assisted prediction of the total; the second term adds an inverse-probability–weighted correction using observed residuals. Under MAR with positivity and if either the response model $\rho(\mathbf v)$ or the outcome model $m(\mathbf v)$ is correctly specified, $\widehat T_{DR}$ is consistent for $T_y$—a survey-sampling adaptation of augmented inverse-probability weighting \parencite{robins_rotnitzky_zhao_1994_jasa,bang_robins_2005_biometrics,haziza_rao_2006_surveymeth,kim_park_2006_cjs,kim_haziza_2014_sinica}.  When the assisting function $f(\mathbf z)$ is available from external training, one may also adopt the PPD–DR form
\begin{equation}
  \widehat T_y^{\mathrm{PPD\text{-}DR}}
  \;=\;
  \sum_{i\in U} f(\mathbf z_i)
  \;+\;
  \sum_{i\in s} d_i\Bigl\{\tilde y_i - f(\mathbf z_i)\Bigr\}
  \;+\;
  \sum_{i\in S_r}\frac{d_i}{\hat\rho_i}\,\bigl(y_i-\tilde y_i\bigr),
  \label{eq:PPD-DR}
\end{equation}
which reduces to \eqref{eq:PPD} if $\hat\rho_i\equiv 1$, to the standard GREG/difference estimator when $r_i\equiv1$, and to the design-weighted AIPW total \eqref{eq:DR} when $f\equiv 0$ (see \textsection\ref{sec:ppd-aipw} for detailed algebra and connections).

\paragraph{Multiply robust (MR) estimators.}
MR procedures extend double robustness by allowing several candidate outcome and/or response models and preserving consistency if \emph{any one} model in the set is correct. A convenient form retains the imputation structure
\begin{equation}
  \widehat T_{MR}
  \;=\;
  \sum_{i\in S_r} d_i\,y_i
  \;+\;
  \sum_{i\in S_m} d_i\,\tilde y_i^{\,MR},
  \label{eq:MR}
\end{equation}
where the imputed values $\tilde y_i^{\,MR}$ are constructed (e.g., via calibrated estimating equations or fractional imputation) to satisfy moment conditions implied by multiple working models for $E(y\mid\mathbf v)$ and/or $\rho(\mathbf v)$; if at least one such model is correctly specified, bias is eliminated even when the others are misspecified \parencite{han_wang_2013_biometrika,han_2014_jasa,chen_haziza_2017_biometrika,chen_haziza_2019_sinica}. These methods trade computational complexity for enhanced robustness and are especially pertinent when guarding against model uncertainty is paramount.

\medskip\noindent\textbf{Remarks on modeling and auxiliary information.}
\emph{(i) Training.} Propensity models $\hat\rho_i$ are typically fit on all sampled units $(r_i,\mathbf v_i)$ with design weights; outcome models $\tilde y_i=\widehat m_s(\mathbf v_i)$ are fit on respondents $(y_i,\mathbf v_i)$ with design weights; assisting functions $f(\mathbf z_i)$ may be trained externally on frame or historical data and treated as fixed in the current draw. \emph{(ii) Assumptions.} IPW requires correct $\rho(\mathbf v)$ and positivity; outcome-model imputation requires mean-correct $m(\mathbf v)$; DR requires (MAR, positivity) and correctness of at least one of $\rho(\mathbf v)$ or $m(\mathbf v)$; calibration/GREG/PPD requires suitable auxiliary coverage and (for exact model-unbiasedness) a correct linear assisting model, though GREG is also approximately design-unbiased when $\mathbf z$ is predictive of $y$ \parencite{deville_sarndal_1992_jasa,sarndal_lundstrom_2005_nonresponse}. \emph{(iii) Variance.} For replication or linearization variance estimation, refit any data-driven components ($\widehat m_s$, $\hat\rho$) within each replicate; if $f(\mathbf z)$ is externally trained on the frame, treat $\sum_{i\in U} f(\mathbf z_i)$ as fixed across replicates \parencite{sarndal_lundstrom_2005_nonresponse,kim_haziza_2014_sinica}.

